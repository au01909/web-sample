<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Keep all existing head content unchanged -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real Time Action Recognition System</title>
    <link rel="icon" type="image/x-icon" href="favicon.ico">
    <style>
        /* Keep all existing CSS styles unchanged */
        body { margin: 0; padding: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif; background-color: #f5f5f5; text-align: justify; overflow-y: auto; }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(20px); } to { opacity: 1; transform: translateY(0); } }
        .header { background-color: #001845; padding: 2rem 0; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); animation: fadeIn 1s ease-in-out; }
        /* ... (keep all other CSS rules exactly as they are) ... */
    </style>
</head>
<body>
    <!-- Keep all existing HTML structure -->
    <header class="header">
        <div class="header-content">
            <div class="header-title">
                <h1>Real Time Action Recognition System</h1>
                <p>AI-Powered Human Activity Recognition</p>
            </div>
        </div>
    </header>

    <div class="container">
        <h2 class="page-title">Project Overview</h2>
        <div class="main-section">
            <div class="content">
                <p>
                    Human Activity Recognition (HAR) using AI is vital for applications like surveillance, healthcare, and sports, 
                    enhancing public safety and automation. This project leverages deep learning and computer vision to develop a 
                    robust real-time system for recognizing human actions in surveillance footage. Using datasets like Weizmann for 
                    initial training, the model is enhanced through data augmentation with multi-action scenarios. The architecture 
                    combines CNN for feature extraction, LSTM for temporal action understanding, and pose estimation for improved 
                    accuracy. To optimize real-time performance, lightweight neural networks and GPU acceleration are employed, 
                    ensuring high accuracy, low latency, and robustness in dynamic environments through rigorous benchmark testing.
                </p>
            </div>
            <div class="image-container">
                <img src="samplee.jpeg" alt="Sample Project Image">
            </div>
        </div>

        <div class="video-container">
            <video id="video-stream" autoplay playsinline></video>
        </div>

        <div class="button-container">
            <button id="start-button" aria-label="Start Surveillance">Start Surveillance</button>
            <button id="stop-button" aria-label="Stop Surveillance">Stop Surveillance</button>
        </div>

        <div id="log-container">
            <h3>Action Log</h3>
            <ul id="log-list"></ul>
        </div>
    </div>

    <!-- Modified Script Section -->
    <script>
        const videoStream = document.getElementById('video-stream');
        const startButton = document.getElementById('start-button');
        const stopButton = document.getElementById('stop-button');
        const logList = document.getElementById('log-list');
        let mediaStream = null;
        let isProcessing = false;

        async function processFrame() {
            if (!mediaStream || !isProcessing) return;

            const canvas = document.createElement('canvas');
            canvas.width = videoStream.videoWidth;
            canvas.height = videoStream.videoHeight;
            canvas.getContext('2d').drawImage(videoStream, 0, 0);

            try {
                const blob = await new Promise(resolve => canvas.toBlob(resolve, 'image/jpeg', 0.8));
                const formData = new FormData();
                formData.append('frame', blob, 'frame.jpg');

                const response = await fetch('/predict', {
                    method: 'POST',
                    body: formData
                });
                
                const prediction = await response.json();
                handlePrediction(prediction);
            } catch (error) {
                console.error('Prediction error:', error);
            }

            if (isProcessing) requestAnimationFrame(processFrame);
        }

        function handlePrediction(prediction) {
            const isViolence = prediction.violence > 0.8;
            videoStream.style.border = isViolence ? '4px solid red' : '2px solid #002366';
            
            if (isViolence) {
                const timestamp = new Date().toLocaleTimeString();
                logList.innerHTML += `<li>${timestamp} - Violence detected (${Math.round(prediction.violence*100)}%)</li>`;
            }
        }

        // Start Surveillance
        startButton.addEventListener('click', async () => {
            try {
                if (!mediaStream) {
                    mediaStream = await navigator.mediaDevices.getUserMedia({ video: true });
                    videoStream.srcObject = mediaStream;
                    logList.innerHTML += '<li>Surveillance started</li>';
                    isProcessing = true;
                    requestAnimationFrame(processFrame);
                }
            } catch (error) {
                console.error('Error accessing webcam:', error);
                logList.innerHTML += '<li>Error accessing webcam</li>';
            }
        });

        // Stop Surveillance
        stopButton.addEventListener('click', () => {
            if (mediaStream) {
                isProcessing = false;
                mediaStream.getTracks().forEach(track => track.stop());
                videoStream.srcObject = null;
                mediaStream = null;
                logList.innerHTML += '<li>Surveillance stopped</li>';
            }
        });
    </script>
</body>
</html>
